{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9949d585-2de6-4f3b-9a91-1b574a6cb37a",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence Model Training with LSTM\n",
    "\n",
    "## Project Overview: Sequence-to-Sequence Model for Language Translation\n",
    "\n",
    "In this project, my aim is to build a **Sequence-to-Sequence (Seq2Seq) neural network** using Long Short-Term Memory (LSTM) units to perform language translation. The Seq2Seq model is a specific type of Recurrent Neural Network (RNN) architecture that I’m using to tackle a common natural language processing task—translating text from one language to another. This approach is not only employed for translation but also in tasks like text summarisation and question answering.\n",
    "\n",
    "## Project Objectives\n",
    "The primary objective of this project is to train a neural network that can translate a given sequence of text in one language (the input) into another language (the output). To achieve this, I’m implementing an **encoder-decoder architecture**. This architecture encodes an input sequence into a fixed-size context vector, which the decoder then uses to generate an output sequence in a different language.\n",
    "\n",
    "## Model Architecture\n",
    "The model I’m using consists of two core components:\n",
    "\n",
    "1. **Encoder**: This part processes the input sequence. It uses an LSTM layer to read the entire input and summarise it into a context vector. I use the final hidden and cell states from the encoder as the initial states for the decoder.\n",
    "2. **Decoder**: The decoder takes the context vector from the encoder and generates the output sequence. It’s also an LSTM network, but it outputs a probability distribution over the tokens of the target language at each step.\n",
    "\n",
    "## Key Features and Steps\n",
    "- **Data Preprocessing**: I prepare the training data by tokenising and encoding the input and output sequences.\n",
    "- **Hyperparameter Definition**: I define key parameters such as the number of hidden units (latent dimensions), batch size, and number of training epochs to fine-tune the model.\n",
    "- **Model Compilation and Training**: I utilise the `rmsprop` optimiser and a `categorical_crossentropy` loss function, which are well-suited for multi-class prediction tasks.\n",
    "- **Model Evaluation**: I include a validation split during training to monitor the model’s accuracy and performance on unseen data.\n",
    "\n",
    "## Applications\n",
    "The Seq2Seq model I’m building can be applied in various real-world scenarios, such as:\n",
    "- **Language Translation**: Translating text between different languages.\n",
    "- **Text Summarisation**: Condensing long documents into shorter summaries.\n",
    "- **Chatbots and Conversational Agents**: Generating human-like responses for automated chat systems.\n",
    "\n",
    "By the end of this project, my goal is to have a trained Seq2Seq model capable of translating sentences from a source language to a target language using the encoder-decoder architecture I designed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5585214e-4bba-46c1-94cb-c1e6199149f5",
   "metadata": {},
   "source": [
    "## 1. Importing Required Libraries and Variables\n",
    "First, I import the necessary data and modules from the `preprocessing` module. This includes information about the number of tokens in the encoder and decoder, as well as the data for training inputs and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dad03da8-26e8-4561-a10f-af5f3da8389e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '$', ',', '.', '00', '17', '18', '19', '2', '30', '300', '50', '8', ':', '?', 'A', 'Abandon', 'Act', 'After', 'Aim', 'All', 'Am', 'Answer', 'Anybody', 'Anyone', 'Anything', 'Are', 'Arm', 'Arrive', 'Ask', 'Attack', 'Awesome', 'BMW', 'Back', 'Be', 'Bear', 'Beat', 'Beef', \"Beer's\", 'Begin', 'Behave', 'Beware', 'Birds', 'Bless', 'Blood', 'Boil', 'Boston', 'Bottoms', 'Boys', 'Break'] Agáchense\n",
      "1752\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import num_encoder_tokens, num_decoder_tokens, decoder_target_data, encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85c6aa1-d648-484a-8418-3e503f34e341",
   "metadata": {},
   "source": [
    "I then import the Keras library from TensorFlow, and specifically add the Input, LSTM, and Dense layers along with the Model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83278750-3f44-4422-b01a-37240ea7a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "# Add Dense to the imported layers\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4b2f92-c76e-4042-8491-e1db73cdfe8a",
   "metadata": {},
   "source": [
    "## 2. Handling Potential Mac Errors\n",
    "If I am running this code on a Mac, I might run into an error due to a duplicated library issue. To avoid this, I set the KMP_DUPLICATE_LIB_OK environment variable to 'True'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "887c6232-7310-4dd9-a5f1-c432aebada95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee589df9-02e5-47b4-8aa4-6fbe66065d2a",
   "metadata": {},
   "source": [
    "## 3. Setting Hyperparameters\n",
    "I define the dimensionality of the LSTM's internal state (latent space) as 256. This means the hidden state of my LSTM will have 256 dimensions. I also set the batch size to 50 and the number of epochs for training to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca863464-d9ca-4285-a75f-4d74e30bf880",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256\n",
    "batch_size = 50\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b60da58-8d04-4450-9d13-223014e0d076",
   "metadata": {},
   "source": [
    "## 4. Encoder Setup\n",
    "For the encoder, I define an input layer that expects a sequence of unspecified length (None), with each element of the sequence having a dimension equal to the number of encoder tokens (num_encoder_tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7c37547-76ac-4509-a785-d35edddee450",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a63275-bcbb-407d-9219-f9fb33f1dcd3",
   "metadata": {},
   "source": [
    "I then define an LSTM layer for the encoder with the specified latent dimension. I also specify that I want the layer to return both the hidden state and the cell state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1774aabb-8859-47bf-9893-29c6bccab939",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_hidden, state_cell]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fa794c-bea0-49b2-af87-952c3226dac8",
   "metadata": {},
   "source": [
    "## 5. Decoder Setup\n",
    "For the decoder, I define a similar input layer for the decoder tokens. I then set up another LSTM layer that takes the encoder's final hidden and cell states as its initial states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce76902e-9b0a-42a5-9241-d2552dc5bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state=encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab3be92-7f6c-42b5-964d-27c7a5afcf4e",
   "metadata": {},
   "source": [
    "Next, I define a Dense layer with a number of units equal to the number of decoder tokens (num_decoder_tokens) and use a softmax activation function to output a probability distribution for each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8211779-75df-4c24-98c6-c33c4298beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05300972-ec99-406e-8553-9fac5af77702",
   "metadata": {},
   "source": [
    "## 6. Building the Model\n",
    "I construct the training model by linking the encoder and decoder input layers to the output of the decoder. This will be the main model I use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c06b4d65-83d1-4a4e-a1aa-de540d4353ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a151d14-ea8b-44b4-b7e8-7ae628b3eb00",
   "metadata": {},
   "source": [
    "I print a summary of the model architecture to visualize its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01bafe39-f6ea-45e2-88d3-43e82a3aecd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1752</span>)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3728</span>)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,057,216</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,080,640</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],       │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]        │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">958,096</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3728</span>)             │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m1752\u001b[0m)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3728\u001b[0m)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │  \u001b[38;5;34m2,057,216\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │  \u001b[38;5;34m4,080,640\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],       │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]        │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │    \u001b[38;5;34m958,096\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│                     │ \u001b[38;5;34m3728\u001b[0m)             │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,095,952</span> (27.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,095,952\u001b[0m (27.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,095,952</span> (27.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,095,952\u001b[0m (27.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Model summary:\\n\")\n",
    "training_model.summary()\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ff94db-91b9-42e6-8567-74200673dcd7",
   "metadata": {},
   "source": [
    "## 7. Compiling the Model\n",
    "Before training, I compile the model with the rmsprop optimizer and a categorical cross-entropy loss function, which is suitable for multi-class classification. I also track the accuracy as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "567ee4e9-8d12-45ae-8982-bb02b1c908eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02138263-252c-423b-9276-efc4a68416a5",
   "metadata": {},
   "source": [
    "## 8. Training the Model\n",
    "I use the fit method to train the model, feeding it the encoder and decoder input data along with the target data. I also set a validation split of 20% to monitor the model's performance on unseen data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "680a7571-2768-4e41-87e2-48090760823b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 189ms/step - accuracy: 0.0607 - loss: 1.9045 - val_accuracy: 0.0728 - val_loss: 1.5400\n",
      "Epoch 2/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 190ms/step - accuracy: 0.0840 - loss: 1.2910 - val_accuracy: 0.0818 - val_loss: 1.4902\n",
      "Epoch 3/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 193ms/step - accuracy: 0.0866 - loss: 1.2453 - val_accuracy: 0.0831 - val_loss: 1.5165\n",
      "Epoch 4/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 186ms/step - accuracy: 0.0916 - loss: 1.2113 - val_accuracy: 0.0906 - val_loss: 1.4757\n",
      "Epoch 5/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 194ms/step - accuracy: 0.0948 - loss: 1.1769 - val_accuracy: 0.0906 - val_loss: 1.4635\n",
      "Epoch 6/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 191ms/step - accuracy: 0.0974 - loss: 1.1640 - val_accuracy: 0.0908 - val_loss: 1.4558\n",
      "Epoch 7/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 212ms/step - accuracy: 0.0998 - loss: 1.1444 - val_accuracy: 0.0937 - val_loss: 1.4469\n",
      "Epoch 8/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 215ms/step - accuracy: 0.1043 - loss: 1.1146 - val_accuracy: 0.0967 - val_loss: 1.4082\n",
      "Epoch 9/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 198ms/step - accuracy: 0.1085 - loss: 1.0987 - val_accuracy: 0.1016 - val_loss: 1.4058\n",
      "Epoch 10/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 218ms/step - accuracy: 0.1129 - loss: 1.0766 - val_accuracy: 0.1117 - val_loss: 1.3542\n",
      "Epoch 11/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 202ms/step - accuracy: 0.1190 - loss: 1.0467 - val_accuracy: 0.1050 - val_loss: 1.3762\n",
      "Epoch 12/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 198ms/step - accuracy: 0.1234 - loss: 1.0229 - val_accuracy: 0.1180 - val_loss: 1.3168\n",
      "Epoch 13/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 203ms/step - accuracy: 0.1257 - loss: 0.9997 - val_accuracy: 0.1135 - val_loss: 1.3180\n",
      "Epoch 14/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 205ms/step - accuracy: 0.1278 - loss: 0.9830 - val_accuracy: 0.1237 - val_loss: 1.2792\n",
      "Epoch 15/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 199ms/step - accuracy: 0.1304 - loss: 0.9559 - val_accuracy: 0.1228 - val_loss: 1.2629\n",
      "Epoch 16/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 205ms/step - accuracy: 0.1321 - loss: 0.9397 - val_accuracy: 0.1319 - val_loss: 1.2413\n",
      "Epoch 17/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 213ms/step - accuracy: 0.1342 - loss: 0.9288 - val_accuracy: 0.1233 - val_loss: 1.2552\n",
      "Epoch 18/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 215ms/step - accuracy: 0.1368 - loss: 0.9048 - val_accuracy: 0.1103 - val_loss: 1.2981\n",
      "Epoch 19/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 213ms/step - accuracy: 0.1379 - loss: 0.8903 - val_accuracy: 0.1252 - val_loss: 1.2278\n",
      "Epoch 20/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 205ms/step - accuracy: 0.1399 - loss: 0.8744 - val_accuracy: 0.1343 - val_loss: 1.1982\n",
      "Epoch 21/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 202ms/step - accuracy: 0.1416 - loss: 0.8513 - val_accuracy: 0.1357 - val_loss: 1.1975\n",
      "Epoch 22/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 202ms/step - accuracy: 0.1426 - loss: 0.8406 - val_accuracy: 0.1388 - val_loss: 1.1957\n",
      "Epoch 23/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 206ms/step - accuracy: 0.1445 - loss: 0.8322 - val_accuracy: 0.1382 - val_loss: 1.1882\n",
      "Epoch 24/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 199ms/step - accuracy: 0.1451 - loss: 0.8284 - val_accuracy: 0.1375 - val_loss: 1.1921\n",
      "Epoch 25/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 203ms/step - accuracy: 0.1458 - loss: 0.8119 - val_accuracy: 0.1391 - val_loss: 1.1751\n",
      "Epoch 26/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 201ms/step - accuracy: 0.1465 - loss: 0.7963 - val_accuracy: 0.1397 - val_loss: 1.1725\n",
      "Epoch 27/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 215ms/step - accuracy: 0.1479 - loss: 0.7960 - val_accuracy: 0.1425 - val_loss: 1.1681\n",
      "Epoch 28/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 231ms/step - accuracy: 0.1484 - loss: 0.7749 - val_accuracy: 0.1421 - val_loss: 1.1631\n",
      "Epoch 29/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 233ms/step - accuracy: 0.1506 - loss: 0.7656 - val_accuracy: 0.1446 - val_loss: 1.1561\n",
      "Epoch 30/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 207ms/step - accuracy: 0.1518 - loss: 0.7610 - val_accuracy: 0.1410 - val_loss: 1.1972\n",
      "Epoch 31/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 211ms/step - accuracy: 0.1527 - loss: 0.7606 - val_accuracy: 0.1456 - val_loss: 1.1389\n",
      "Epoch 32/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 213ms/step - accuracy: 0.1532 - loss: 0.7405 - val_accuracy: 0.1453 - val_loss: 1.1343\n",
      "Epoch 33/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 212ms/step - accuracy: 0.1546 - loss: 0.7315 - val_accuracy: 0.1458 - val_loss: 1.1338\n",
      "Epoch 34/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 210ms/step - accuracy: 0.1557 - loss: 0.7219 - val_accuracy: 0.1450 - val_loss: 1.1303\n",
      "Epoch 35/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 219ms/step - accuracy: 0.1565 - loss: 0.7111 - val_accuracy: 0.1460 - val_loss: 1.1290\n",
      "Epoch 36/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 219ms/step - accuracy: 0.1582 - loss: 0.6999 - val_accuracy: 0.1475 - val_loss: 1.1214\n",
      "Epoch 37/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 214ms/step - accuracy: 0.1581 - loss: 0.6918 - val_accuracy: 0.1488 - val_loss: 1.1167\n",
      "Epoch 38/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 214ms/step - accuracy: 0.1583 - loss: 0.6843 - val_accuracy: 0.1454 - val_loss: 1.1249\n",
      "Epoch 39/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 212ms/step - accuracy: 0.1597 - loss: 0.6760 - val_accuracy: 0.1490 - val_loss: 1.1221\n",
      "Epoch 40/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 212ms/step - accuracy: 0.1605 - loss: 0.6652 - val_accuracy: 0.1443 - val_loss: 1.1333\n",
      "Epoch 41/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 213ms/step - accuracy: 0.1604 - loss: 0.6589 - val_accuracy: 0.1477 - val_loss: 1.1122\n",
      "Epoch 42/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 215ms/step - accuracy: 0.1624 - loss: 0.6522 - val_accuracy: 0.1497 - val_loss: 1.1195\n",
      "Epoch 43/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 212ms/step - accuracy: 0.1622 - loss: 0.6439 - val_accuracy: 0.1493 - val_loss: 1.1229\n",
      "Epoch 44/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 213ms/step - accuracy: 0.1622 - loss: 0.6316 - val_accuracy: 0.1504 - val_loss: 1.1060\n",
      "Epoch 45/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 216ms/step - accuracy: 0.1641 - loss: 0.6291 - val_accuracy: 0.1506 - val_loss: 1.1085\n",
      "Epoch 46/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 208ms/step - accuracy: 0.1658 - loss: 0.6206 - val_accuracy: 0.1513 - val_loss: 1.1070\n",
      "Epoch 47/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 211ms/step - accuracy: 0.1671 - loss: 0.6071 - val_accuracy: 0.1487 - val_loss: 1.1313\n",
      "Epoch 48/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 218ms/step - accuracy: 0.1661 - loss: 0.6030 - val_accuracy: 0.1519 - val_loss: 1.1014\n",
      "Epoch 49/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 214ms/step - accuracy: 0.1683 - loss: 0.5958 - val_accuracy: 0.1518 - val_loss: 1.1079\n",
      "Epoch 50/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 212ms/step - accuracy: 0.1684 - loss: 0.5912 - val_accuracy: 0.1519 - val_loss: 1.1078\n",
      "Epoch 51/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 216ms/step - accuracy: 0.1693 - loss: 0.5804 - val_accuracy: 0.1512 - val_loss: 1.1139\n",
      "Epoch 52/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 206ms/step - accuracy: 0.1683 - loss: 0.5768 - val_accuracy: 0.1516 - val_loss: 1.1011\n",
      "Epoch 53/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 226ms/step - accuracy: 0.1690 - loss: 0.5725 - val_accuracy: 0.1536 - val_loss: 1.1078\n",
      "Epoch 54/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 245ms/step - accuracy: 0.1701 - loss: 0.5635 - val_accuracy: 0.1535 - val_loss: 1.1115\n",
      "Epoch 55/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 240ms/step - accuracy: 0.1708 - loss: 0.5481 - val_accuracy: 0.1520 - val_loss: 1.1161\n",
      "Epoch 56/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 227ms/step - accuracy: 0.1711 - loss: 0.5411 - val_accuracy: 0.1535 - val_loss: 1.1067\n",
      "Epoch 57/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 217ms/step - accuracy: 0.1733 - loss: 0.5461 - val_accuracy: 0.1546 - val_loss: 1.1080\n",
      "Epoch 58/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 214ms/step - accuracy: 0.1732 - loss: 0.5372 - val_accuracy: 0.1535 - val_loss: 1.1097\n",
      "Epoch 59/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 214ms/step - accuracy: 0.1728 - loss: 0.5297 - val_accuracy: 0.1560 - val_loss: 1.1033\n",
      "Epoch 60/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 269ms/step - accuracy: 0.1746 - loss: 0.5197 - val_accuracy: 0.1549 - val_loss: 1.1114\n",
      "Epoch 61/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 428ms/step - accuracy: 0.1744 - loss: 0.5219 - val_accuracy: 0.1537 - val_loss: 1.1123\n",
      "Epoch 62/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 429ms/step - accuracy: 0.1780 - loss: 0.5088 - val_accuracy: 0.1531 - val_loss: 1.1191\n",
      "Epoch 63/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 422ms/step - accuracy: 0.1761 - loss: 0.5058 - val_accuracy: 0.1563 - val_loss: 1.1220\n",
      "Epoch 64/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 430ms/step - accuracy: 0.1785 - loss: 0.4979 - val_accuracy: 0.1551 - val_loss: 1.1088\n",
      "Epoch 65/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 431ms/step - accuracy: 0.1781 - loss: 0.4892 - val_accuracy: 0.1562 - val_loss: 1.1152\n",
      "Epoch 66/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 402ms/step - accuracy: 0.1792 - loss: 0.4951 - val_accuracy: 0.1559 - val_loss: 1.1319\n",
      "Epoch 67/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 409ms/step - accuracy: 0.1776 - loss: 0.4803 - val_accuracy: 0.1551 - val_loss: 1.1350\n",
      "Epoch 68/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 416ms/step - accuracy: 0.1794 - loss: 0.4844 - val_accuracy: 0.1555 - val_loss: 1.1305\n",
      "Epoch 69/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 418ms/step - accuracy: 0.1801 - loss: 0.4738 - val_accuracy: 0.1543 - val_loss: 1.1429\n",
      "Epoch 70/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 413ms/step - accuracy: 0.1808 - loss: 0.4688 - val_accuracy: 0.1545 - val_loss: 1.1251\n",
      "Epoch 71/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 405ms/step - accuracy: 0.1819 - loss: 0.4603 - val_accuracy: 0.1548 - val_loss: 1.1313\n",
      "Epoch 72/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 422ms/step - accuracy: 0.1824 - loss: 0.4681 - val_accuracy: 0.1555 - val_loss: 1.1286\n",
      "Epoch 73/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 423ms/step - accuracy: 0.1820 - loss: 0.4577 - val_accuracy: 0.1546 - val_loss: 1.1303\n",
      "Epoch 74/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 458ms/step - accuracy: 0.1824 - loss: 0.4590 - val_accuracy: 0.1560 - val_loss: 1.1247\n",
      "Epoch 75/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 488ms/step - accuracy: 0.1847 - loss: 0.4428 - val_accuracy: 0.1570 - val_loss: 1.1299\n",
      "Epoch 76/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 416ms/step - accuracy: 0.1857 - loss: 0.4364 - val_accuracy: 0.1571 - val_loss: 1.1312\n",
      "Epoch 77/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 448ms/step - accuracy: 0.1848 - loss: 0.4283 - val_accuracy: 0.1570 - val_loss: 1.1275\n",
      "Epoch 78/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 429ms/step - accuracy: 0.1870 - loss: 0.4302 - val_accuracy: 0.1563 - val_loss: 1.1388\n",
      "Epoch 79/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 439ms/step - accuracy: 0.1857 - loss: 0.4266 - val_accuracy: 0.1558 - val_loss: 1.1375\n",
      "Epoch 80/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 436ms/step - accuracy: 0.1865 - loss: 0.4246 - val_accuracy: 0.1557 - val_loss: 1.1459\n",
      "Epoch 81/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 441ms/step - accuracy: 0.1859 - loss: 0.4189 - val_accuracy: 0.1555 - val_loss: 1.1458\n",
      "Epoch 82/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 464ms/step - accuracy: 0.1891 - loss: 0.4154 - val_accuracy: 0.1542 - val_loss: 1.1679\n",
      "Epoch 83/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 489ms/step - accuracy: 0.1886 - loss: 0.4119 - val_accuracy: 0.1556 - val_loss: 1.1519\n",
      "Epoch 84/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 443ms/step - accuracy: 0.1907 - loss: 0.4096 - val_accuracy: 0.1553 - val_loss: 1.1530\n",
      "Epoch 85/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 426ms/step - accuracy: 0.1903 - loss: 0.3999 - val_accuracy: 0.1563 - val_loss: 1.1506\n",
      "Epoch 86/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 420ms/step - accuracy: 0.1914 - loss: 0.4006 - val_accuracy: 0.1540 - val_loss: 1.1535\n",
      "Epoch 87/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 428ms/step - accuracy: 0.1914 - loss: 0.3921 - val_accuracy: 0.1548 - val_loss: 1.1624\n",
      "Epoch 88/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 427ms/step - accuracy: 0.1918 - loss: 0.3870 - val_accuracy: 0.1550 - val_loss: 1.1685\n",
      "Epoch 89/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 427ms/step - accuracy: 0.2037 - loss: 0.3847 - val_accuracy: 0.1544 - val_loss: 1.1688\n",
      "Epoch 90/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 429ms/step - accuracy: 0.1950 - loss: 0.3771 - val_accuracy: 0.1582 - val_loss: 1.1623\n",
      "Epoch 91/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 446ms/step - accuracy: 0.1933 - loss: 0.3784 - val_accuracy: 0.1553 - val_loss: 1.1740\n",
      "Epoch 92/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 440ms/step - accuracy: 0.1929 - loss: 0.3690 - val_accuracy: 0.1542 - val_loss: 1.1753\n",
      "Epoch 93/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 434ms/step - accuracy: 0.1942 - loss: 0.3677 - val_accuracy: 0.1576 - val_loss: 1.1688\n",
      "Epoch 94/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 478ms/step - accuracy: 0.1955 - loss: 0.3579 - val_accuracy: 0.1571 - val_loss: 1.1671\n",
      "Epoch 95/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 484ms/step - accuracy: 0.1957 - loss: 0.3595 - val_accuracy: 0.1574 - val_loss: 1.1811\n",
      "Epoch 96/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 434ms/step - accuracy: 0.1963 - loss: 0.3546 - val_accuracy: 0.1522 - val_loss: 1.1901\n",
      "Epoch 97/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 439ms/step - accuracy: 0.1959 - loss: 0.3515 - val_accuracy: 0.1557 - val_loss: 1.1891\n",
      "Epoch 98/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 444ms/step - accuracy: 0.1965 - loss: 0.3517 - val_accuracy: 0.1592 - val_loss: 1.1825\n",
      "Epoch 99/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 427ms/step - accuracy: 0.1978 - loss: 0.3348 - val_accuracy: 0.1575 - val_loss: 1.1900\n",
      "Epoch 100/100\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 424ms/step - accuracy: 0.2112 - loss: 0.3386 - val_accuracy: 0.1573 - val_loss: 1.1896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x140be8940>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size = batch_size, epochs = epochs, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1d2516-ac1b-4d3f-8d87-c0f87be266a8",
   "metadata": {},
   "source": [
    "## 9. Saving the Model\n",
    "After training is complete, I save the model to a file named 'training_model.h5'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b965cd9-a08a-43ad-ac55-ca625d5c5d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.save('training_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdaf11a-4042-4d8c-981a-664c4312e404",
   "metadata": {},
   "source": [
    "This completes the training process for a basic sequence-to-sequence model using an encoder-decoder architecture with LSTM layers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
