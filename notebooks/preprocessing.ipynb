{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41833ba0-b2da-41a7-9de5-78a585c66197",
   "metadata": {},
   "source": [
    "# Project Overview: Data Preprocessing for Sequence-to-Sequence Language Translation Model\n",
    "\n",
    "In this project, I aim to prepare a dataset for training a **Sequence-to-Sequence (Seq2Seq) neural network** for language translation tasks. Before training the model, I need to preprocess the text data to create structured inputs and outputs suitable for a machine learning model. This preprocessing step includes tokenising the text, building vocabulary sets, and encoding sentences into numerical formats that can be fed into the neural network.\n",
    "\n",
    "The dataset Iâ€™m using contains pairs of sentences in two languages, separated by tabs, with each pair representing a translation from one language to another. My task is to process this data and convert it into a format that the Seq2Seq model can work with efficiently.\n",
    "\n",
    "## Project Objectives\n",
    "The objective of this project is to:\n",
    "- Read and split the text dataset into input and target sentences.\n",
    "- Build vocabulary sets for both the input and target sentences.\n",
    "- Tokenise the sentences and add special tokens such as `<START>` and `<END>` to the target sentences.\n",
    "- Convert each token into a unique numerical representation.\n",
    "- Structure the processed data into three main components: encoder inputs, decoder inputs, and decoder targets, to be used in training the Seq2Seq model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f818693-c03c-49c6-8830-c3b6d9741d16",
   "metadata": {},
   "source": [
    "## Code Breakdown and Explanation\n",
    "\n",
    "### 1. Importing Libraries and Reading the Dataset\n",
    "First, I import the necessary libraries and set the path to the dataset file. I read the file, split it by lines, and prepare to process each line separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8afd600b-c39b-458a-a0a2-b145490f7bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Importing our translations\n",
    "# for example: \"spa.txt\" or \"spa-eng/spa.txt\"\n",
    "data_path = \"spa.txt\"\n",
    "\n",
    "# Defining lines as a list of each line\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "  lines = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d8d4ff-19a2-4c52-ba74-d950d55bff06",
   "metadata": {},
   "source": [
    "## 2. Initialising Variables and Preparing Lists\n",
    "I create empty lists to store the input and target sentences. Additionally, I initialise two sets to hold the unique tokens from both the input and target sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a825dd5c-c647-4e18-bd8b-a46d4543f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_docs = []\n",
    "target_docs = []\n",
    "input_tokens = set()\n",
    "target_tokens = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec5f189-1e61-477c-a5e1-543cbb240336",
   "metadata": {},
   "source": [
    "## 3. Processing Each Line from the Dataset\n",
    "Here, I process each line up to a defined limit (6000 lines in this example) to avoid long preprocessing times. I split each line into input and target sentences using a tab separator.\n",
    "\n",
    "I add a <START> token at the beginning and an <END> token at the end of each target sentence. I then append these sentences to the corresponding lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d90e2ed1-6b05-4c39-b121-14489a855fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines[:6001]:\n",
    "  input_doc, target_doc = line.split('\\t')[:2]\n",
    "  input_docs.append(input_doc)\n",
    "\n",
    "  target_doc = \" \".join(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc))\n",
    "  target_doc = '<START> ' + target_doc + ' <END>'\n",
    "  target_docs.append(target_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df6e66d-fd3e-47a6-afe6-f47866e12785",
   "metadata": {},
   "source": [
    "## 4. Tokenising Sentences and Building Vocabulary Sets\n",
    "For each sentence, I split it into words or tokens. I add each unique token to the corresponding vocabulary set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f977d469-b764-44fd-820d-31b56d223606",
   "metadata": {},
   "outputs": [],
   "source": [
    "  for token in re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc):\n",
    "    if token not in input_tokens:\n",
    "      input_tokens.add(token)\n",
    "  for token in target_doc.split():\n",
    "    if token not in target_tokens:\n",
    "      target_tokens.add(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd7d764-fd3b-4cde-9e86-3965d6177915",
   "metadata": {},
   "source": [
    "## 5. Creating Sorted Lists of Tokens and Defining Features\n",
    "I sort the tokens and count the number of unique tokens in both input and target vocabularies. I then define dictionaries that map each token to a unique index and create reverse mappings for decoding purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e6d05c65-7318-4fe0-af0e-3885b3b7163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = sorted(list(input_tokens))\n",
    "target_tokens = sorted(list(target_tokens))\n",
    "\n",
    "num_encoder_tokens = len(input_tokens)\n",
    "num_decoder_tokens = len(target_tokens)\n",
    "\n",
    "max_encoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)) for input_doc in input_docs])\n",
    "max_decoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc)) for target_doc in target_docs])\n",
    "\n",
    "input_features_dict = dict(\n",
    "    [(token, i) for i, token in enumerate(input_tokens)])\n",
    "target_features_dict = dict(\n",
    "    [(token, i) for i, token in enumerate(target_tokens)])\n",
    "\n",
    "reverse_input_features_dict = dict(\n",
    "    (i, token) for token, i in input_features_dict.items())\n",
    "reverse_target_features_dict = dict(\n",
    "    (i, token) for token, i in target_features_dict.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3d334d-1487-41ff-82d5-372b02b49439",
   "metadata": {},
   "source": [
    "## 6. Preparing Data for the Neural Network\n",
    "I initialise three-dimensional arrays to store the encoder and decoder inputs, as well as the decoder targets. These arrays are sized according to the number of sentences and their maximum lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9e48200a-07db-445a-9bcd-190b24eeb251",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_docs), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_docs), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_docs), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2adc22-5896-4c73-b6bf-eb40590c808e",
   "metadata": {},
   "source": [
    "## 7. Encoding Sentences into Numerical Data\n",
    "For each line in the dataset, I convert each token in the sentences into its corresponding index based on the previously defined dictionaries. This process is done separately for the encoder inputs and the decoder inputs and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "82aaabc1-bd9a-4e96-8045-446b42f3d2b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Go'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line, (input_doc, target_doc) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(input_docs, target_docs)):\n\u001b[1;32m      3\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m timestep, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]+|[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw]\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_doc)):\n\u001b[0;32m----> 4\u001b[0m     encoder_input_data[line, timestep, \u001b[43minput_features_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m timestep, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(target_doc\u001b[38;5;241m.\u001b[39msplit()):\n\u001b[1;32m      7\u001b[0m     decoder_input_data[line, timestep, target_features_dict[token]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Go'"
     ]
    }
   ],
   "source": [
    "for line, (input_doc, target_doc) in enumerate(zip(input_docs, target_docs)):\n",
    "\n",
    "  for timestep, token in enumerate(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)):\n",
    "    encoder_input_data[line, timestep, input_features_dict[token]] = 1.\n",
    "\n",
    "  for timestep, token in enumerate(target_doc.split()):\n",
    "    decoder_input_data[line, timestep, target_features_dict[token]] = 1.\n",
    "    if timestep > 0:\n",
    "      decoder_target_data[line, timestep - 1, target_features_dict[token]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d17e2b-72bf-4c9d-9c69-2cff299e9633",
   "metadata": {},
   "source": [
    "## 8. Displaying Token Mappings\n",
    "Finally, I print a subset of the input tokens and one of the reverse target tokens as a check to ensure that the mappings are accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3664eb4e-09e0-4039-bc3e-e7521cf897a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "50",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlist\u001b[39m(input_features_dict\u001b[38;5;241m.\u001b[39mkeys())[:\u001b[38;5;241m50\u001b[39m], \u001b[43mreverse_target_features_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(input_tokens))\n",
      "\u001b[0;31mKeyError\u001b[0m: 50"
     ]
    }
   ],
   "source": [
    "print(list(input_features_dict.keys())[:50], reverse_target_features_dict[50])\n",
    "print(len(input_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c3e3dc-cc81-477c-81bf-db855953d2ff",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this code, Iâ€™ve performed the crucial step of data preprocessing for a Seq2Seq model. By reading, cleaning, and tokenising text data, Iâ€™ve created the necessary input and output structures that the model will use during training. The data has been transformed into numerical arrays that can be efficiently processed by a neural network. This preprocessed data forms the foundation for building a translation model in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f762e1-9306-426c-bf89-c8d45dabd28e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
