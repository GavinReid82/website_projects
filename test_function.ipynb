{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d032b5e-6cea-4f27-9ff7-ae8091490829",
   "metadata": {},
   "source": [
    "# Project Overview: Inference and Decoding for Sequence-to-Sequence Language Translation Model\n",
    "\n",
    "In this part of the project, my goal is to implement the inference mechanism for the trained **Sequence-to-Sequence (Seq2Seq) model**. After successfully training the model, I aim to build and utilise separate encoder and decoder models to generate translations. This phase involves reconstructing the trained model in a way that allows me to use it for decoding new, unseen input sentences.\n",
    "\n",
    "The approach taken here leverages the trained weights and structure of the Seq2Seq model to predict translations, effectively implementing a **sampling process** using the trained encoder-decoder architecture.\n",
    "\n",
    "## Project Objectives\n",
    "The objectives in this phase include:\n",
    "- Loading the trained model to extract its encoder and decoder parts separately.\n",
    "- Building the inference models for both the encoder and the decoder.\n",
    "- Implementing a decoding function that generates translated sentences from the input using a sampling method.\n",
    "- Testing the model with sample input sentences and displaying the translated output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38799995-cad0-4c2c-b5c3-a2c5cc630619",
   "metadata": {},
   "source": [
    "## Code Breakdown and Explanation\n",
    "\n",
    "### 1. Importing Modules and Loading the Trained Model\n",
    "I start by importing relevant modules and preprocessed data variables. I also load the trained Seq2Seq model from the file `'training_model.h5'`. After loading the model, I extract the encoder’s input and output states from the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a8b338-bb56-47ca-81ef-c27795d8a016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import input_features_dict, target_features_dict, reverse_input_features_dict, reverse_target_features_dict, max_decoder_seq_length, input_docs, target_docs, input_tokens, target_tokens\n",
    "from training_model import encoder_inputs, decoder_inputs, encoder_states, decoder_lstm, decoder_dense, encoder_input_data, num_decoder_tokens, latent_dim\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model, load_model\n",
    "import numpy as np\n",
    "\n",
    "training_model = load_model('training_model.h5')\n",
    "encoder_inputs = training_model.input[0]\n",
    "encoder_outputs, state_h_enc, state_c_enc = training_model.layers[2].output\n",
    "encoder_states = [state_h_enc, state_c_enc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4c702c-b819-4da5-b753-49e3a7ea294a",
   "metadata": {},
   "source": [
    "## 2. Building the Encoder Model\n",
    "Using the loaded model, I create a separate encoder model that takes the encoder input and outputs its internal states (state_h and state_c). This model will be used to encode new input sentences during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd7b53e-10c8-450e-ba88-171e73f15b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc7f81-378d-4d50-b5e7-0fb1fb790414",
   "metadata": {},
   "source": [
    "## 3. Building the Decoder Model\n",
    "Next, I define the decoder model for inference. I start by creating input placeholders for the decoder’s hidden and cell states. The decoder takes these states as inputs, along with the actual decoder inputs, to produce the predicted token and the updated states. I then define the complete decoder model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c0658a-a4a6-44e7-923d-78541be76165",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
    "decoder_state_input_cell = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]\n",
    "decoder_outputs, state_hidden, state_cell = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_hidden, state_cell]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf224d-b60e-4b0e-8ca6-ac93d83863b3",
   "metadata": {},
   "source": [
    "## 4. Decoding Function for Generating Translations\n",
    "Here, I define the decode_sequence function, which takes an encoded input and generates the corresponding translated sentence using the trained decoder. The function:\n",
    "\n",
    "- Encodes the input sentence into state vectors using the encoder model.\n",
    "- Initialises an empty target sequence and sets the first token to the <START> token.\n",
    "- Repeatedly runs the decoder to predict the next token until an <END> token is generated or the maximum sequence length is reached.\n",
    "- Accumulates the decoded tokens to build the translated sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf33aa1-676b-4227-81d6-56c572715f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(test_input):\n",
    "  states_value = encoder_model.predict(test_input)\n",
    "\n",
    "  target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "  target_seq[0, 0, target_features_dict['<START>']] = 1.\n",
    "\n",
    "  decoded_sentence = ''\n",
    "  stop_condition = False\n",
    "  while not stop_condition:\n",
    "    output_tokens, hidden_state, cell_state = decoder_model.predict(\n",
    "      [target_seq] + states_value)\n",
    "\n",
    "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "    sampled_token = reverse_target_features_dict[sampled_token_index]\n",
    "    decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "    if (sampled_token == '<END>' or len(decoded_sentence) > max_decoder_seq_length):\n",
    "      stop_condition = True\n",
    "\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "    states_value = [hidden_state, cell_state]\n",
    "\n",
    "  return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5388a1-6a11-4004-a655-e690da8e88c4",
   "metadata": {},
   "source": [
    "## 5. Testing the Model with Input Sentences\n",
    "I run the decoding function on a set of test input sentences to check the performance of the model. Here, I choose a range of input sentences and display their decoded outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3858e5-b4bd-4fec-a16f-28b23c4a7681",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in range(100):\n",
    "  test_input = encoder_input_data[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(test_input)\n",
    "  print('-')\n",
    "  print('Input sentence:', input_docs[seq_index])\n",
    "  print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b562df-3762-4237-8170-350606d36c65",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this phase of the project, I’ve set up the inference mechanism for the Seq2Seq model. I successfully:\n",
    "\n",
    "- Extracted and built separate models for the encoder and decoder from the trained model.\n",
    "- Created a decoding function that generates translations using a sampling method.\n",
    "- Tested the model by decoding a batch of input sentences and displaying their translated outputs.\n",
    "\n",
    "With this, I am now able to use the trained Seq2Seq model to generate translations for new input sentences, completing the implementation of the language translation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f573d120-e051-4bf7-addb-9e2f3df9880e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
